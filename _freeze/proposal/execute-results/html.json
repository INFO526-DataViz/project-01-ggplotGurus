{
  "hash": "b7d0d054d1a768eab97cff27f17e8508",
  "result": {
    "markdown": "---\ntitle: \"USA Weather Forecast Accuracy Analysis\"\nsubtitle: \"Proposal\"\nformat: html\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(\"pacman\"))\n  install.packages(\"pacman\")\npacman::p_load(tidyverse, tidytuesdayR)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npackage 'credentials' successfully unpacked and MD5 sums checked\npackage 'zip' successfully unpacked and MD5 sums checked\npackage 'gitcreds' successfully unpacked and MD5 sums checked\npackage 'httr2' successfully unpacked and MD5 sums checked\npackage 'ini' successfully unpacked and MD5 sums checked\npackage 'desc' successfully unpacked and MD5 sums checked\npackage 'gert' successfully unpacked and MD5 sums checked\npackage 'gh' successfully unpacked and MD5 sums checked\npackage 'rprojroot' successfully unpacked and MD5 sums checked\npackage 'whisker' successfully unpacked and MD5 sums checked\npackage 'usethis' successfully unpacked and MD5 sums checked\npackage 'tidytuesdayR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n\tC:\\Users\\sweth\\AppData\\Local\\Temp\\Rtmpg1js2z\\downloaded_packages\n```\n:::\n:::\n\n\n> ## Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_forecasts <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/weather_forecasts.csv')\ncities <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/cities.csv')\noutlook_meanings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-12-20/outlook_meanings.csv')\n```\n:::\n\n\nThis dataset represents Weather Forecast Accuracy and is sourced from the [USA National Weather Service](https://www.weather.gov/). You can check out the dataset in [Tidytuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-12-20) It encompasses 16 months of weather forecasts and subsequent observations for 167 cities across the United States. Weather_forecast_Accuracy is divided into three dimensions, `weather_forecasts`, `cities`, and `outlook_meanings`.\n\nThe first dataset we'll look at is `weather_forecasts`, which contains 651,968 observations of 10 variables. Specifically, this includes factor variables like `city`, `state` and `forecast_outlook` or integer variables like temperature and forecast hours. The second dataset we'll look at is `cities`, which contains 236 observations of 11 variables with geographic information such as latitude, longitude, and climate for 236 cities, including the 167 cities we'll be looking at. The last dataset, `outlook_meanings`, contains 23 observations of 2 variables and is the one that describes the aforementioned variable, `forecast_outlook`.\n\n------------------------------------------------------------------------\n\n> ## Why did we choose this dataset\n\nOur Data Visualization class project requires us to work collaboratively on a dataset of our choice to create meaningful visualizations.\n\nAfter careful consideration, we have chosen to work with a weather forecast dataset, for several compelling reasons:\n\n1.  Richness and Depth: The dataset holds an extensive range of information, covering 16 months of forecasts and observations from 167 cities. With variables including temperature, humidity, precipitation, and wind speed, among others, this dataset offers an opportunity to visualize and analyze diverse aspects of weather patterns, enabling a comprehensive understanding of meteorological trends.\n2.  Learning Opportunities: Working with weather forecast data challenges us to apply the data visualization skills we acquired in class to a complex and dynamic dataset. This hands-on experience will undoubtedly enhance our proficiency in data analysis and visualization.\n3.  Historical connections:Weather has been one of the earliest uses of data visualization techniques. Early as 1875 Sir Francis Galton created the daily weather chart that was published in The Times newspaper. We aim to continue this legacy by being part of the innovation, in data visualization. Through this project we strive to push the boundaries of creating weather related data in todays era\n4.  Weather Intelligence Industry: Finally, our choice was motivated by the currently thriving weather intelligence industry. In today's data-driven world, companies like Tomorrow.io and EarthNetworks.comhave created a demand for precise and clear weather data and insights. This not only aligns with industry needs, but also provides us with a chance to enhance our data analysis and visualization skills in a highly relevant context.\n\nIn summary, we are excited about the potential of our weather forecast data project and are committed to producing exceptional visualizations. We look forward to using this dataset to create informative and engaging visualizations that showcase our skills and contribute meaningfully to the field of data visualization.\n\n------------------------------------------------------------------------\n\n> ## Questions\n\n-   Question 1: How does the error in temperature prediction vary across the United States? and can we identify clusters of cities with similar error patterns?\n\n-   Question 2: How do temperature forecasting errors vary across the U.S. during different seasons, and can we visualize this on a geographic heatmap?\n\n------------------------------------------------------------------------\n\n> ## Analysis Plan\n\n### **Question 1: Spatial Distribution of Temperature Forecasting Errors** {style=\"color: blue\"}\n\nHow does the error in temperature prediction vary across the United States? and can we identify clusters of cities with similar error patterns?\n\nThis question is essential since errors in weather forecasting are the main reason for this data set to be created. To answer the question, we plan on using advanced geospatial data visualization techniques. To represent the data best, we will create an interactive choropleth map of the United States that displays the spatial distribution of temperature forecasting errors for U.S. cities. This will be done by using the package leaflet and taking the following steps:\n\n-   Choropleth Map: It will be generated using the variables:\n\n    -   Latitude (type: double)\n    -   Longitude (type: double)\n    -   Cities (type: charecter)\n    -   Temperature forecasting errors (type: factor)\n\n-   Temperature forecasting errors: Below are the variables that will be used to calculate temperature forecasting errors:\n\n    -   Observed Temperature (Degrees Fahrenheit).\n    -   Forecasted Temperature (Degrees Fahrenheit).\n\n-   Clustering Cities Based on Regional Error Patterns:\n\n    -   Categorize cities by state.\n    -   Calculate Regional Averages\n\nFinally, we will implement a dark mode design to achieve a modern and eye-catching aesthetic, and we will utilize the color theory to show the results. Also, in the data visualization we are creating as a heatmap, we will attempt to make it possible for users to be able to zoom in and out for a detailed exploration of specific regions. We hope that this visualization will not only provide answers to the scientific questions but also engage users with modern and interactive design elements, enhancing their understanding of the data and its implications.\n\n### **Question 2: Geographic Seasonality in Temperature Forecasting Errors** {style=\"color: blue\"}\n\nHow do temperature forecasting errors vary across the U.S. during different seasons, and can we visualize this on a geographic heatmap?\n\nThe objective is to determine whether certain regions in the U.S. exhibit seasonal variations in temperature forecast errors. For this, we'll use heatmaps overlaid on a map of the U.S.\n\nPlans for answering the question 2:\n\n-   Variables Involved:\n\n    -   date (type: integer) : To determine the season (e.g., Winter, Spring, Summer, Fall).\n\n    -   city and state ( type: factor) : For geographical mapping.\n\n    -   observed_temp and forecast_temp( type: integer) : To calculate the error.\n\n-   Variables to be Created:\n\n    -   season (type: charecter) : Derived from the date variable.\n\n    -   temp_error (type: factor) : The difference between observed_temp and forecast_temp.\n\n    -   mean_temp_error_per_season (type: factor) : Average error for each city during each season.\n\n-   External Data:\n\n    -   No external data is required at this time. However, if deeper insights are needed in the future regarding specific climate anomalies or unique geographical features not captured in the current dataset, external sources might be sought.\n\n-   Analysis and Visualization:\n\n    -   Begin by segregating the data seasonally.\n\n    -   Compute the error for each observation and then average them seasonally for each city.\n\n    -   Plot this data on a geographical heatmap using the latitude and longitude data available in the cities dataset. The heatmap will showcase regions with the highest temperature forecasting errors, with distinct visuals for each season.\n\nBy the end of this analysis, we aim to have a clear visualization that depicts regions with consistently high forecasting errors and to observe if there's a seasonality to these errors across different parts of the U.S.\n",
    "supporting": [
      "proposal_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}